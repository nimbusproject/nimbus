m4_include(/mcs/m4/worksp.lib.m4)
_NIMBUS_HEADER(2.3 Cloud Guide)
_NIMBUS_HEADER2(n,n,y,n,n,n,n)
_NIMBUS_LEFT2_COLUMN
_NIMBUS_LEFT2_ADMIN2_SIDEBAR
_NIMBUS_LEFT2_COLUMN_END
_NIMBUS_CENTER2_COLUMN
_NIMBUS_TP2_2_DEPRECATED

<div class="ullispace"> <!-- surrounds all of the doc -->

<h2>Cloud Guide (2.3)</h2>
        
<p>
    This page describes a particular configuration of Nimbus
    that allows the cloud-client to operate out of the box.  If you've never
    configured Nimbus before, you should be able to follow this
    page conceptually but it is not meant to be a replacement for the
    <a href="../admin/index.html">administrator guide</a> which will still need
    to be consulted.
</p>

<p class="indent">
    <img src="../../../img/warning.gif" alt="warning!" class="floatleft" />
    <i>This page is for <b>deployers</b> of the cloud configuration to
    learn about it and configure the workspace service for it.  This is
    <b>not necessary for cloud users</b> to read and understand.</i>  If you
    are a cloud user just looking to understand how to launch and manage VMs
    on an existing cloud, start at the <a href="../clouds/">clouds page</a>.
</p>

<p>
    <i>Table of Contents</i>
</p>

<ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#userexp">User Experience</a></li>
    <li><a href="#assumptions">Assumptions and Defaults</a></li>
    <li><a href="#necessary">Necessary Configurations</a></li>
    <li><a href="#props">Properties and Options</a></li>
    <li><a href="#security">Security</a></li>
    <li><a href="#cloud-admin">Cloud admin program</a></li>
    <li><a href="#configs">Configuration Appendix</a></li>
</ul>

<a name="overview"> </a>
<h2>Overview _NAMELINK(overview)</h2>

<p>
    The service must be set up in
    <a href="../admin/reference.html#resource-pool-and-pilot">resource pool
    mode</a>, controlling any number of
    VMM nodes.  You may use the workspace
    <a href="../admin/reference.html#resource-pool-and-pilot">pilot</a> to integrate with a
    local resource scheduler.  An image repository must be set up, this will
    host workspace image files for each client.  When a client runs a workspace,
    the image to use is transferred from the repository to the VMM that will
    be running it.
</p>

<p>
    For the sake of discussion we will assume that the workspace service and
    file repository setup are on different nodes.  This does not necessarily
    need to be the case but it is the recommended configuration because of the
    heavy I/O traffic the repository can experience.
</p>
<p>
    The workspace service should be <a href="../admin/index.html">installed</a> as
    normal on the service node and <a href="http://www.gridftp.org">GridFTP</a>
    must be installed on the repository node.
</p>
<p>
    The server addresses must be directly reachable from the Internet
    or otherwise configured to deal with being NAT'd.  The Globus container
    (where the workspace service runs) and GridFTP can both be setup for
    NAT or other port forwarding situations.
</p>
<p>
    <img src="../img/cloud-overview-rt.png" alt="workspace cloud configuration" />
</p>
<p>
    The diagram above depicts the basic setup.
</p>
<ul>
    <li>
        A special workspace client called the "cloud-client" invokes operations
        on the service and GridFTP server.  A number of defaults are assumed
        which makes this work out of the box (these defaults will be discussed
        later).
    </li>
    <li>
        Files are transferred from the cloud-client to a client-specific
        directory on the repository node (manual or other types of GridFTP
        based transfers are also possible if the user is comfortable with
        using grid tools directly).
    </li>
    <li>
        The service invokes commands on the VMMs to trigger file transfers
        to/from the repository node, VM lifecycle events, and destruction/clean
        up.
    </li>
    <li>
        If the workspace state changes, the cloud-client will reflect this to
        the screen (and log files) and depending on the change might also take
        action in response.
    </li>
</ul>



<a name="userexp"> </a>
<h2>User Experience _NAMELINK(userexp)</h2>

<p>
    Working backwards from the user's <i>cloud-client</i> experience is a
    good way to understand how the service needs to be setup.
</p>

<p>
    Here is an abbreviated depiction of a simple user interaction with a cloud,
    to give you an idea if you've never used it.  This does not depict an
    image transfer to the repository node but that is similarly brief.
</p>

<ol>
    <li>
        <p>
            A grid credential is needed, there is an embedded
            <i>grid-proxy-init</i> program if that is necessary.
        </p>
    </li>

    <li>
        <p>
            You can list what's in your repository directory:
        </p>

<div class="screen">
<b>$</b> ./bin/cloud-client.sh --list<br>
</div>
        <p>
            Sample output:
        </p>

<div class="screen">
<pre>[Image] 'base-cluster-01.gz'       Read only
        Modified: Jul 06 @ 17:34   Size: 578818017 bytes (~552 MB)

[Image] 'globus-002'               Read only
        Modified: Jun 12 @ 18:55   Size: 3758097408 bytes (~3584 MB)

[Image] 'hello-cloud'              Read only
        Modified: May 30 @ 14:16   Size: 524288000 bytes (~500 MB)

[Image] 'hello-cluster'            Read only
        Modified: Jun 30 @ 20:18   Size: 524288000 bytes (~500 MB)</pre>
</div>


    </li>

    <li>
        <p>
            And pick one to run (ignore the 'cluster' images for now)
        </p>

<div class="screen">
<b>$</b> ./bin/cloud-client.sh --run --name hello-cloud --hours 1<br>
</div>
        <p>
            Sample output:
        </p>

<div class="screen"><pre>
SSH public keyfile contained tilde:
  - '~/.ssh/id_rsa.pub' --> '/home/guest/.ssh/id_rsa.pub'

Launching workspace.

Using workspace factory endpoint:
    https://cloudurl.edu:8443/wsrf/services/WorkspaceFactoryService

Creating workspace "vm-023"... done.

       IP address: 123.123.123.123
         Hostname: ahostname.cloudurl.edu
       Start time: Fri Feb 29 09:36:39 CST 2008
    Shutdown time: Fri Feb 29 10:36:39 CST 2008
 Termination time: Fri Feb 29 10:46:39 CST 2008

Waiting for updates.
</pre></div>

        <p>
            Some time elapses as the image file is copied to the VMM node. Then
            a running notification is printed:
        </p>

<div class="screen"><pre>
State changed: Running

Running: 'vm-023'
</pre></div>
    </li>

    <li>
        <p>
            The client had picked up your default public SSH key and sent it to
            be installed on the fly into the VM's <i>authorized_keys</i> policy
            for the root account.  So after launching you can use the printed
            hostname to log in as root:
        </p>

        <div class="screen">
            <b>$</b> ssh root@ahostname.cloudurl.edu<br>
        </div>
    </li>
</ol>

<br />

<p>
    You can see an example of a cluster cloud-client deployment on the
    <a href="../clouds/clusters.html">one-click clusters</a> page.
</p>

<p>
    So how does this happen?
</p>

<a name="assumptions"> </a>
<h2>Assumptions and Defaults _NAMELINK(assumptions)</h2>

<p>
    A number of things go into making the cloud client work out of the box,
    but it is in large part accomplished by giving the user a downloadable
    package with a number of default configurations.
</p>

<p>
    These defaults limit functionality options in some cases, but that is the
    idea: eliminate decisions that need to be made and set working defaults.
    There are avenues left open for experienced users to do more
    (for example, by overriding the defaults or even switching over to the
    regular workspace client).
</p>

<p>
    In the previous section, the first thing that probably stands out is that
    there are no contact addresses being entered on the command line.
</p>

<p>
    The service and repository URLs are derived from a properties file
    that is included in the toplevel "conf" directory of the cloud-client
    package.  An example file is this
    <a href="../examples/cloud.properties">cloud.properties file</a> which
    is currently distributed for the Nimbus cloud.
</p>

<p class="indent">
    <b>Note</b>: How properties files and commandline overrides work is covered
    in a <a href="#props">later section</a> in detail, it is all designed to be
    flexible under the covers.  If you don't want to follow the conventions
    laid out in this current "assumptions" section, it will be important to
    understand the later section to know how to change things for a good
    client package or properties file(s) that your users can use.  Continue
    reading this section first, though, to get the basic ideas.
</p>

<p>
    There are three main groups of assumptions and defaults.  The first is the
    contact and identity information of the workspace service and GridFTP
    server (see above for configuration sample where this are specified).
    The other two groups make up the rest of this "Assumptions" section:
</p>

<ul>
    <li><a href="#personaldir">Deriving per-user repository directories</a></li>
    <li><a href="#runtime">Runtime assumptions</a></li>
</ul>


<a name="personaldir"> </a>
<h3>Deriving per-user repository directories _NAMELINK(personaldir)</h3>

<p>
    For GridFTP based commands (like <i>--list</i>, <i>--delete</i>, and
    <i>--transfer</i>) the server to contact is based on the contact in
    the cloud properties file.  The X509 identity to verify is in the
    cloud properties file.  If that property was missing, identity checks
    would be based on hostname.
</p>

<p class="indent">
    <i>Remember that we are not going to discuss the various ways of
       getting options in this "Assumptions" section.</i>
</p>

<p>
    When you transfer a local file, the target of the transfer is the same
    filename in your personal repository directory. When you refer to the name
    of a workspace to run, this name must correspond to a filename in your
    personal repository directory.
</p>

<p>
    We know where the repository comes from but how is that directory derived?
</p>

<p>
    There are two other components to derive the directory used: the configured
    <i>base directory property</i> and the <i>hash of the caller's X509
    Distinguished Name</i>.
</p>
<ul>
    <li>
        The configured <i>base directory property</i>.  The default
        configuration for the base directory on the repository node is
        "<b>/cloud</b>".
    </li>
    <li>
        A <i>hash of the caller's X509 Distinguished Name</i> is used as
        the subdirectory of the base directory.  The algorithm for this
        is based on MD5.  It produces a string of eight characters, for
        example "<b>31ceb17f</b>".  The <i>credential being used for the
        call</i> is inspected to get the user's DN.
    </li>
</ul>

<p>
    The directories for each user are created by the administrator.  Any
    (unlikely) hash collisions would be detected at this point.  You can
    see the hash of any "Globus style" DN with the <i>--hash-print</i> option
    of the cloud client.  For example:
</p>


<div class="screen">
<b>$</b> ./bin/cloud-client.sh --hash-print "/DC=org/DC=agrid/OU=people/CN=John Q. Public"<br>
</div>

<p>
    Sample output:
</p>

<div class="screen">
<pre>DN: /DC=org/DC=agrid/OU=people/CN=John Q. Public
HASH: a9bad55</pre>
</div>

<p>
    So with a hypothetical repository hostname "repository.cloudurl.edu",
    "/cloud" base directory and DN hash of "a9bad55", the derived GridFTP URL
    of the user's "my-workspace" file will be
    <i>gsiftp://repository.cloudurl.edu:2811//cloud/a9bad55/my-workspace</i>
</p>

<p class="indent">
    <i>
        Note that there is a cloud-client option to input any name or local
        file path and see what the derived URL is.  See the --extrahelp
        description of the --print-file-URL option.
    </i>
</p>

<p>
    <b>As of TP2.2, you can auto-create the user directories using the
    <a href="#cloud-admin">cloud-admin</a> program.</b>
</p>

<a name="runtime"> </a>
<h3>Runtime assumptions _NAMELINK(runtime)</h3>

<p>
    The second set of assumptions to cover is how a given image file is going
    to actually work.  There are many options that you can specify in regular
    workspace requests.  For example, the memory size, the number of network 
    interfaces to construct, the pool name(s) to lease network addresses from,
    and the partition name the VM is expecting for the base partition.
</p>

<p>
    Some fixed assumptions are made:
</p>
<ul>
    <li>There can be only <b>one network interface</b></li>
    <li>The network interface <b>is expecting its address via DHCP</b></li>
    <li>
        There can be only <b>one partition file</b>, for the root partition,
        configured with an ext2/ext3 filesystem.  Other filesystems may not
        work correctly (this has to do with the cloud's default kernel as well
        as its ability to edit the image's files before boot).
    </li>
</ul>
<p>
    The rest of the launch request is filled by default configurations,
    here they are:
</p>
<ul>
    <li>Request <b>3584</b> MB of memory</li>
    <li>Request networking address from a pool named <b>public</b></li>
    <li>Mount the partition to <b>sda1</b></li>
</ul>



<a name="necessary"> </a>
<h2>Necessary Configurations _NAMELINK(necessary)</h2>

<p>
    The previous section summed up the defaults and main assumptions.  Opting
    to follow these conventions in your cloud leads to these configuration
    conclusions:
</p>

<ul>
    <li>
        <p>
            Install the workspace service in
            <a href="../admin/reference.html#resource-pool-and-pilot">resource
            pool mode</a>.
        </p>
    </li>
    <li>
        <p>
            Configure an
            <a href="../admin/quickstart.html#networks">network</a>
            for addresses to lease from and call it "<b>public</b>".
        </p>
    </li>
    <li>
        <p>
            Create a <i>cloud.properties</i> file for your cloud with
            the values in this <a href="../examples/cloud.properties">example
            file</a> changed to reflect the correct URLs and identities.
        </p>
    </li>
    <li>
        <p>
            If you need to adjust the default memory request, add a line of
            text like so to the <i>cloud.properties</i> file you will
            distribute:  <b>vws.memory.request=2560</b>
        </p>
    </li>
    <li>
        <p>
            Create a <i>/cloud</i> directory on the repository node.
        </p>
    </li>
    <li>
        <p>
            For each user, take the hash of their DN (using --hash-print)
            and create a directory for them under the <i>/cloud</i> base
            directory.
        </p>
    </li>
    
</ul>


<a name="props"> </a>
<h2>Properties and Options _NAMELINK(props)</h2>

<p>
    This section goes into more detail about the property file and commandline
    configurations.  This is especially important to understand if you want
    to diverge from the defaults above.
</p>

<div>

    <img src="../img/cloud-client-layout.png"
         alt="workspace cloud client layout"
         class="floatright" />

    <p>
        All commands go through <i>cloud-client.sh</i> which in turn
        invokes the actual cloud client program.  The cloud client is written
        in Java and installed at <i>lib/globus/lib/workspace_client.jar</i>.
    </p>
    <p>
        Before calling this program, the script sets up some things:
    </p>

    <ul>
        <li>
            <i>../conf/cloud.properties</i> is set as the user properties file
        </li>
        <li>
            <i>../lib/globus</i> becomes the new GLOBUS_LOCATION (overriding
            anything previously set)
        </li>
        <li>
            <i>../lib/certs</i> is set as a directory to add to the trusted
            X509 certificate directories for identity validations (the client
            verifies it is talking to the right servers).  Adding the CA cert(s)
            of the workspace service and GridFTP host certificates to this
            directory ensures that the user will not run into CA (trusted
            certificates) problems.
        </li>
    </ul>
<p>
    The cloud client program respects settings from <b>three</b> different
    places, listed here in the <b>order of precedence</b>:
</p>
<ul>
    <li>
        <p>
            <b>Commandline arguments</b> - If the client uses one of the
            optional flags listed in <i>./bin/cloud-client.sh --extrahelp</i>,
            these values are used.  Many things can be overriden this way,
            including the service contacts.
        </p>
    </li>
    <li>
        <p>
            <b>User properties file</b> - An example of this was given
            above (the <a href="../examples/cloud.properties">cloud.properties
            file</a> which is currently distributed for the
            <a href="/clouds/">Nimbus</a> cloud).
        </p>
        <p>            
            Note that you can include different properties files and have your
            users switch between clouds using
            <i>./bin/cloud-client.sh --conf ./conf/some-file</i>.
        </p>
        <p>
            If no <i>--conf</i> argument is supplied, the default file
            <i>cloud.properties</i> needs to exist.  If you need to change
            this in your client distribution for cosmetic reasons, you can
            do so by editing the one relevant line at the top of
            <i>./bin/cloud-client.sh</i>
        </p>
    </li>
    <li>
        <p>
            <b>Embedded properties file</b> - A properties file lives inside
            the workspace client jar (which is installed into
            <i>lib/globus/lib/workspace_client.jar</i>).  This controls all
            the remaining configurations.
        </p>
    </li>
</ul>
</div>

<p>
    There are (intentionally) <b>no fallback settings</b> for the properties
    found in that sample
    <a href="../examples/cloud.properties">cloud.properties file</a>:
</p>

<ul>
    <li>ssh.pubkey (Path to SSH public key to log in with)</li>
    <li>vws.factory (Host+port of Virtal Workspace Service)</li>
    <li>vws.factory.identity (Virtal Workspace Service X509 identity)</li>
    <li>vws.repository (Host+port of image repository)</li>
    <li>vws.repository.identity (Image repository X509 identity)</li>
</ul>

<p>
    See the configuration <a href="#configs">appendix</a> for other, more
    esoteric defaults that can be tampered with.
</p>


<a name="clusters"> </a>
<h2>Clusters _NAMELINK(clusters)</h2>

<p>
    To enable one-click clusters, you need to enable the context broker (see
    <a href="../admin/reference.html#context-broker">this section</a> admin
    guide).
</p>


<a name="security"> </a>
<h2>Security _NAMELINK(security)</h2>

<p>
    The <a href="../plugins/index.html">plugins page</a> discusses the
    "groupauthz" plugin which provides for many generally useful policies to
    be enforced, but one in particular is necessary for the cloud configuration
    to operate properly.  The identity-hash based image subdirectories option
    ensures that propagation source paths and unpropagation target paths are
    specific to the caller using the hashing algorithm discussed above.
</p>

<p>
    The workspace-control user account is empowered to run all workspaces,
    so this authorization of specific requests is necessary before the "enactment"
    command is sent out to workspace-control, work done on behalf of the client
    but importantly not <b>as the client</b>.
</p>

<p>
    For the repository node you currently need
    <a href="http://www.gridftp.org">GridFTP</a> to handle remote transfers.
    Each cloud user's DN must be in the GridFTP grid-mapfile (an access control
    list that also maps each DN to a specific unix account).  In order to
    prevent users from maliciously overwriting each others files when talking
    to GridFTP directly, currently each cloud user must be mapped to a unique
    unix account which is part of a unique unix group on the repository node.
</p>

<p>    
    The umask of each user should be set to 0007 to allow for group read/write
    permissions on the files.  This allows one control account to be used for
    propagation: in the workspace-control configuration file, there is a
    setting to force it to propagate and unpropagate with one specific user.
    That user needs to be a member of every cloud account's group.  Thus, the
    control user has access to read and write all files (for
    propagation and unpropagation tasks) but each remote client may
    only read and write their personal files.
</p>

<p>
    Say that the base directory on the repository node is "/cloud", you will
    need to create a directory for each DN based on the hash.  For easy tracking
    purposes, the recommendation for the unix account and group is to use the
    hash for each.
</p>

<p>
    Recognizing that this burdens the administrator, we are planning on adding
    SAML based authorization support.  In this configuration, the
    cloud client will (transparent to the human user) get a SAML assertion
    from the workspace service and present it to the GridFTP server for access.
    The SAML authorization statement restricts that client's rights to only
    reading and writing their personal files.
    This allows for a lot less administrative overhead when adding cloud users.
    Currently we are waiting on
    <a href="http://bugzilla.mcs.anl.gov/globus/show_bug.cgi?id=4895">this
    GridFTP work</a> to be completed.
</p>


<a name="cloud-admin"> </a>
<h2>Cloud admin program _NAMELINK(cloud-admin)</h2>

<p>
    As of TP2.2, there is a program installed here:
    <i>$GLOBUS_LOCATION/share/nimbus-autoconfig/cloud-admin.sh</i>
</p>
<p>
    This program can add new users for you with one command, including creating
    the directories with the right hash names.
    During its first "add-dn" invocation, you can set up many
    default choices including what "sample images" get soft linked to the new
    directory, etc.
</p>
<p>
    Here are the current options:
</p>

<div class="screen">
<pre>--add-dn "/CN=Some DN"       Adds new DN (interactive)

--del-dn "/CN=Some DN"       Deletes a DN (interactive)

--find-dn "/CN=Some DN"      Checks for a DN

--hash-dn "/CN=Some DN"      Outputs cloud hash for a DN

--find-hash 1234abcd         Looks in policies for a DN with this hash

--all-dns                    Prints all active DNs

--enable-groupauthz          Enables the groupauthz plugin

--disable-groupauthz         Disables the groupauthz plugin</pre>
</div>

<a name="configs"> </a>
<h2>Configuration Appendix _NAMELINK(configs)</h2>

<p>
    These are the embedded properties that are shipped with the cloud client,
    they can also exist in the cloud properties files to override the defaults:
</p>

<pre>
# Default ms between polls
vws.poll.interval=2000

# Default client behavior is to poll, not use asynchronous notifications
vws.usenotifications=false

# Default memory request
vws.memory.request=3584

# Image repository base directory
vws.repository.basedir=/cloud/

# CA hash of target cloud
vws.cahash=6045a439

# propagation setup for cloud
vws.propagation.scheme=scp
vws.propagation.keepport=false

# GridFTP transfer timeout, 0 is infinite
vws.gridftp.timeout=0


# Metadata defaults
vws.metadata.association=public
vws.metadata.mountAs=sda1
vws.metadata.nicName=eth0
vws.metadata.cpuType=x86
vws.metadata.vmmType=Xen
vws.metadata.vmmVersion=3

# Filename defaults for history directory
vws.metadata.fileName=metadata.xml
vws.depreq.fileName=deprequest.xml
</pre>

</div> <!-- end class="ullispace" from the top -->

_NIMBUS_CENTER2_COLUMN_END
_NIMBUS_FOOTER1
_NIMBUS_FOOTER2
_NIMBUS_FOOTER3
